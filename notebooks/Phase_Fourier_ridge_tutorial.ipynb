{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase Features with ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will demonstrate the use of phase features in distribution regression (for the aerosol dataset), in comparison with the normal Gaussian Kernel (Fourier Features approach), and in a scenario of covariate shift. Parameters were chosen previously with cross validation for this particular set of data and frequencies, through cross validation.\n",
    "\n",
    "For this tutorial, we will make use of the kerpy package from https://github.com/oxmlcs/kerpy.\n",
    "\n",
    "This is tested on Python 2.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Make sure these are on path\n",
    "import numpy as np\n",
    "from kerpy.GaussianKernel import GaussianKernel\n",
    "from kerpy.LinearKernel import LinearKernel\n",
    "from kerpy.LinearBagKernel import LinearBagKernel\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from SymInvBagKernel import SymInvBagKernel\n",
    "import aux_fct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify path for data and load the aerosol dataset (MISR1). One can also normalise data (and normalise labels), though results does not seem to be sensitive to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (800, 100, 16)\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/hochunglaw/Desktop/Fourier-Phase-Neural-Network/data' #Change path \n",
    "misr_data_x, misr_data_y = aux_fct.load_data(path, random = True) \n",
    "print('Data shape:', misr_data_x.shape)\n",
    "# Train Set\n",
    "train_x = misr_data_x[:640]\n",
    "train_y = misr_data_y[:640]\n",
    "# Test Set\n",
    "test_x = misr_data_x[640:]\n",
    "test_y = misr_data_y[640:]\n",
    "variance = np.var(np.concatenate(misr_data_x), axis = 0) # For calculating signal to noise ratio\n",
    "data_dim = misr_data_x.shape[2]\n",
    "mean_predict_t = np.linalg.norm(test_y - np.mean(train_y))**2/len(test_y) #Mean prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the bandwidth for phase features kernel,(note that a larger scale will give you a more robust model to covariate shift, so when tuning, one should choose the larger scale within an acceptable range of error), here we use the same bandwidth + reg parameter for both, so that it is a more fair comparison. Also in general, we expect phase to perform better on smaller dimensional dataset and note that it is more sensitive to the choice of frequencies, relative to fourier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandwidth: 592.091937266\n"
     ]
    }
   ],
   "source": [
    "bandwidth_scale = 2.0\n",
    "bandwidth = np.sqrt(aux_fct.median_sqdist(train_x) / 2) * bandwidth_scale # Calculate bandwidth\n",
    "print('Bandwidth:', bandwidth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Setup the kernel using the kerpy kernel class structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.12649407e-03   4.35964070e-05  -1.31334234e-03 ...,   1.01914805e-04\n",
      "   -1.76231078e-03  -1.70571800e-03]\n",
      " [  7.46060442e-04   1.90659048e-03  -3.10436210e-03 ...,  -2.06598789e-03\n",
      "    2.39037199e-03   7.73040386e-04]\n",
      " [  1.23101802e-03   3.32454237e-03  -9.25173902e-04 ...,  -7.67407436e-05\n",
      "    1.75798032e-03  -1.58817793e-04]\n",
      " ..., \n",
      " [ -9.96534918e-05   1.52940940e-04  -3.30935054e-03 ...,  -3.56494600e-04\n",
      "   -3.47245176e-03   1.71365775e-03]\n",
      " [ -4.21688813e-04  -1.98924139e-03   4.68809121e-04 ...,   1.75583922e-03\n",
      "    1.72603469e-03   2.85188958e-03]\n",
      " [  4.57280433e-04   3.07467721e-03  -5.81815095e-04 ...,   2.64615130e-05\n",
      "    2.90672289e-04   1.75453416e-04]]\n"
     ]
    }
   ],
   "source": [
    "data_phase_kernel = GaussianKernel(sigma=float(bandwidth))\n",
    "phase_kernel = SymInvBagKernel(data_phase_kernel) #Defines the phase features kernel, normalising the fourier features\n",
    "np.random.seed(23)\n",
    "#Generate the frequencies from a normal distribution using the bandwidth, frequencies printed and same as below.\n",
    "phase_kernel.rff_generate(mdata= 250, dim= data_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct explicit feature maps for bags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_phase_means = phase_kernel.rff_expand(train_x)\n",
    "test_phase_means = phase_kernel.rff_expand(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By constructing an explicit feature map for each bag as below, we can now use any normal regression methods on $\\mathbb{R}^{16}$, here we use ridge regression (with a linear kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 0.162785267464\n",
      "Phase RMSE: 0.0693909669971\n"
     ]
    }
   ],
   "source": [
    "lmbda = 0.001 #Ridge regularisation parameter\n",
    "ridge_kernel = LinearKernel()\n",
    "obj_phase, t_predict, t_phase_mse = ridge_kernel.ridge_regress(train_phase_means, train_y, lmbda, test_phase_means, test_y)\n",
    "print('Mean RMSE:', np.sqrt(mean_predict_t))\n",
    "print('Phase RMSE:', np.sqrt(t_phase_mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same for the Gaussian Kernel, using same set of frequencies. Note the LinearKernel() of the data_kernel provides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.12649407e-03   4.35964070e-05  -1.31334234e-03 ...,   1.01914805e-04\n",
      "   -1.76231078e-03  -1.70571800e-03]\n",
      " [  7.46060442e-04   1.90659048e-03  -3.10436210e-03 ...,  -2.06598789e-03\n",
      "    2.39037199e-03   7.73040386e-04]\n",
      " [  1.23101802e-03   3.32454237e-03  -9.25173902e-04 ...,  -7.67407436e-05\n",
      "    1.75798032e-03  -1.58817793e-04]\n",
      " ..., \n",
      " [ -9.96534918e-05   1.52940940e-04  -3.30935054e-03 ...,  -3.56494600e-04\n",
      "   -3.47245176e-03   1.71365775e-03]\n",
      " [ -4.21688813e-04  -1.98924139e-03   4.68809121e-04 ...,   1.75583922e-03\n",
      "    1.72603469e-03   2.85188958e-03]\n",
      " [  4.57280433e-04   3.07467721e-03  -5.81815095e-04 ...,   2.64615130e-05\n",
      "    2.90672289e-04   1.75453416e-04]]\n",
      "Mean RMSE: 0.162785267464\n",
      "Fourier (Gaussian) RMSE: 0.0749136861309\n"
     ]
    }
   ],
   "source": [
    "#This computes the mean embedding (average) of the explicit feature map of the data_kernel (Gaussian), \n",
    "#effectively providing us a gaussian bag kernel. \n",
    "data_gauss_kernel = GaussianKernel(sigma=float(bandwidth))\n",
    "gauss_kernel = LinearBagKernel(data_gauss_kernel)\n",
    "np.random.seed(23)\n",
    "gauss_kernel.rff_generate(mdata= 250, dim= data_dim)\n",
    "train_gauss_means = gauss_kernel.rff_expand(train_x)\n",
    "test_gauss_means = gauss_kernel.rff_expand(test_x)\n",
    "\n",
    "lmbda = 0.001 #Ridge regularisation parameter\n",
    "ridge_kernel = LinearKernel()\n",
    "obj_gauss, t_predict, t_gauss_mse = ridge_kernel.ridge_regress(train_gauss_means, train_y, lmbda, test_gauss_means, test_y)\n",
    "print('Mean RMSE:', np.sqrt(mean_predict_t))\n",
    "print('Fourier (Gaussian) RMSE:', np.sqrt(t_gauss_mse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Fourier features (Gaussian Kernel on Bags) and Phase Features perfrom similarly in this case. Now we demonstrate that phase features are invariant up to Symmetric Positive Definite (SPD) noise, by artficially creating a scenario of covariate shift, where we add __only__ noise to the test set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_ratio_t = 3.0 # This is a super high noise to signal setting.\n",
    "test_x_noisy = np.zeros((160, 100, 16))\n",
    "latent_t = noise_ratio_t * variance * np.random.uniform(low = 0.0, high = 1.0, size = (160, 1, 16))\n",
    "for i in range(160):\n",
    "    for k in range(16):\n",
    "        test_x_noisy[i,:,k] = test_x[i,:,k] + np.sqrt(latent_t[i,:,k]) * np.random.normal(size = (100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the features are very different (for the first sample in first bag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: First sample in first bag\n",
      " [ 372.33903211  273.97752857  222.03223966  206.53149968  347.60454266\n",
      "  265.22960477  237.22911295  193.27649299  421.43038523  274.16365461\n",
      "  232.19638219  194.04713291   21.39239526   38.21109247  116.34367275\n",
      "  130.11828423]\n",
      "Noisy: First sample in first bag\n",
      " [ 298.9780435   249.19810836  192.99434325  234.32955854  331.12292451\n",
      "   71.75804117  253.09135085  340.17637478  332.77734323  279.17942752\n",
      "  133.95235616  119.59060674   51.07726173   11.37218012  122.29624417\n",
      "  126.7050332 ]\n"
     ]
    }
   ],
   "source": [
    "print('Original: First sample in first bag\\n',test_x[0][0])\n",
    "print('Noisy: First sample in first bag\\n', test_x_noisy[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain RMSE on the noisy test set using non-noisy training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean RMSE: 0.162785267464\n",
      "\n",
      "PHASE\n",
      "Phase RMSE: 0.0693909669971\n",
      "Noisy Phase RMSE: 0.119631934894\n",
      "\n",
      "FOURIER\n",
      "Fourier (Gaussian) RMSE: 0.0749136861309\n",
      "Noisy Fourier (Gaussian) RMSE: 0.142921231861\n"
     ]
    }
   ],
   "source": [
    "test_noisy_gauss_means = gauss_kernel.rff_expand(test_x_noisy)\n",
    "test_noisy_phase_means = phase_kernel.rff_expand(test_x_noisy)\n",
    "\n",
    "ridge_kernel = LinearKernel()\n",
    "t_noisy_phase_predict = np.dot(obj_phase.T,ridge_kernel.kernel(train_phase_means,test_noisy_phase_means))\n",
    "t_noisy_gauss_predict = np.dot(obj_gauss.T,ridge_kernel.kernel(train_gauss_means,test_noisy_gauss_means))\n",
    "\n",
    "print('Mean RMSE:', np.sqrt(mean_predict_t))\n",
    "print('\\nPHASE')\n",
    "print('Phase RMSE:', np.sqrt(t_phase_mse))\n",
    "print('Noisy Phase RMSE:', np.sqrt((np.linalg.norm(t_noisy_phase_predict.T - test_y)**2)/len(test_y)))\n",
    "print('\\nFOURIER')\n",
    "print('Fourier (Gaussian) RMSE:', np.sqrt(t_gauss_mse))\n",
    "print('Noisy Fourier (Gaussian) RMSE:', np.sqrt((np.linalg.norm(t_noisy_gauss_predict.T - test_y)**2)/len(test_y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that indeed the Phase features, which are invariant to SPD noise (Gaussian Noise in this case) performs much better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
